# ─────────────────────────────────────────────────────────────
# File: conf/config.yaml  (single flexible config)
# ─────────────────────────────────────────────────────────────
# Keep working directory stable; multirun outputs go here if you sweep later.
hydra:
  run:
    dir: ./misc/submitter_artifacts
  sweep:
    dir: ./misc/submitter_artifacts/multirun/${now:%Y%m%d_%H%M%S}
    subdir: ${hydra.job.num}

# Global toggles
fail_fast: true   # stop on first failed experiment
dry_run: false    # set true to print commands only
jobs: ALL  # comma list or YAML list of exp names; or ALL
local_run: false

# Metadata available to interpolation contexts
meta:
  # run_id: ${now:%y%m%d-%H%M%S}
  run_id: ${now:%y%m%d}
  artifact_root: ./misc/submitter_artifacts


# 1) Shared args for *all* jobs. You can put anything here.
#    Values can be strings, numbers, bools, or lists (lists become comma-joined).
#    Set a key to null in an exp override to remove it from the final CLI.
base_args:
  _cluster_param:
    app: cluster_scripts/cluster.py:train
    host: tc_any
    nnodes: 2
    nproc_per_node: 8
    max_retries: 1
    script: ./cluster_scripts/setup_and_run.sh
    # either use string or list of strings (e.g., ["python", "src/sft/train.py"])
    program: ["python", "src/sft/train.py"]
    config_file: src/sft/configs/train/qwen3_0_6b-full_sft.yaml # can be none
    name: null

  # Paths
  dataset_dir: /mnt/workspace/svg_glyph_llm/data/processed/filtered_sft/250903-oxford_5000-100_fonts-apply_word_sep
  # Default mixture
  dataset: "train-sample_100,train-alphanumeric"
  eval_dataset: "ood_test-sample_30-contents_600,ood_test-alphanumeric"

  # Training hyperparams
  # HF trainer args: https://huggingface.co/docs/transformers/en/main_classes/trainer#transformers.TrainingArguments
  per_device_train_batch_size: 1
  gradient_accumulation_steps: 1
  learning_rate: 1.0e-5
  weight_decay: 0.01

  num_train_epochs: 2.0
  # max_steps: 100000
  # warmup_ratio: 500
  warmup_ratio: 0.1

  # save_strategy: epoch
  save_strategy: steps
  save_steps: 0.1

  eval_strategy: steps
  eval_steps: 0.1 # If smaller than 1, will be interpreted as ratio of total training steps.

  # cutoff_len: 6144

  # add sep token
  add_special_tokens: "<|SEP|>"

  # Output path can depend on exp_name at runtime
  _output_base_dir: /mnt/workspace/svg_glyph_llm/saves/
  # [NOTE] Change this to your own output dir
  _exp_name: 250903-oxford_5000-100_fonts
  _job_name: debug
  output_dir: null # Should be none, override by job_args key

  flash_attn: fa2
  eval_on_start: false
  eval_on_each_dataset: true
  log_level: info

  # [NOTE] be aware of the thinking
  enable_thinking: false

  pure_bf16: true

  disable_gradient_checkpointing: false

# fsdp: https://huggingface.co/docs/transformers/en/main_classes/trainer#transformers.TrainingArguments.fsdp
# "full_shard": Shard parameters, gradients and optimizer states.
# "shard_grad_op": Shard optimizer states and gradients.
# "hybrid_shard": Apply FULL_SHARD within a node, and replicate parameters across nodes.
# "hybrid_shard_zero2": Apply SHARD_GRAD_OP within a node, and replicate parameters across nodes.
# "offload": Offload parameters and gradients to CPUs (only compatible with "full_shard" and "shard_grad_op").
# "auto_wrap": Automatically recursively wrap layers with FSDP using default_auto_wrap_policy.

# 2) Experiment-specific overrides. Each value is merged into base_args.
#    Use null (~) to delete a key from the final argument list.
job_args:
  Qwen3-4B-Instruct-2507:
    model_name_or_path: /mnt/workspace/hf_downloads/Qwen/Qwen3-4B-Instruct-2507
    template: qwen3_nothink

    deepspeed: null
    fsdp: hybrid_shard_zero2 auto_wrap
    fsdp_transformer_layer_cls_to_wrap: Qwen3DecoderLayer

    # dataset info:
    # num_samples: 146354
    # max_seq_len: 1000

    per_device_train_batch_size: 2
    # bs=1, A100, 170 W, 30/30 GB
    # bs=2, A100, 280-330 W, 40/30 GB; w/ckpt + bf16 weight + grad accum, 50 GB; w/ckpt + bf16 weight, 40GB
    # bs=3, A100, 280-330 W, 70/50 GB;
    gradient_accumulation_steps: 2
    _cluster_param:
      nnodes: 1
      nproc_per_node: 8

    # speed up training
    disable_gradient_checkpointing: false

  Qwen3-4B-Base:
    model_name_or_path: /mnt/workspace/hf_downloads/Qwen/Qwen3-4B-Base
    template: qwen3_nothink

    deepspeed: null
    fsdp: hybrid_shard_zero2 auto_wrap
    fsdp_transformer_layer_cls_to_wrap: Qwen3DecoderLayer

    # dataset info:
    # num_samples: 146354
    # max_seq_len: 1000

    per_device_train_batch_size: 2
    # bs=1, A100, 170 W, 30/30 GB
    # bs=2, A100, 280-330 W, 40/30 GB; w/ckpt + bf16 weight + grad accum, 50 GB; w/ckpt + bf16 weight, 40GB
    # bs=3, A100, 280-330 W, 70/50 GB;
    gradient_accumulation_steps: 2
    _cluster_param:
      nnodes: 1
      nproc_per_node: 8

    # speed up training
    disable_gradient_checkpointing: false


  Qwen3-30B-A3B-Instruct-2507:
    model_name_or_path: /mnt/workspace/hf_downloads/Qwen/Qwen3-30B-A3B-Instruct-2507
    template: qwen3_nothink

    deepspeed: null
    fsdp: hybrid_shard auto_wrap
    fsdp_transformer_layer_cls_to_wrap: Qwen3MoeDecoderLayer

    per_device_train_batch_size: 2
    # bs=1, A100, 100 W, 50-70 GB
    # bs=2, A100, 100 W, 60-80 GB; +grad accum, OOM
    gradient_accumulation_steps: 1
    _cluster_param:
      nnodes: 2
      nproc_per_node: 8
    # eval takes about half an hour
    eval_steps: 0.25
    # save per hour
    save_steps: 500

  Qwen3-30B-A3B-Base:
    model_name_or_path: /mnt/workspace/hf_downloads/Qwen/Qwen3-30B-A3B-Base
    template: qwen3_nothink

    deepspeed: null
    fsdp: hybrid_shard auto_wrap
    fsdp_transformer_layer_cls_to_wrap: Qwen3MoeDecoderLayer

    per_device_train_batch_size: 2
    # bs=1, A100, 100 W, 50-70 GB
    # bs=2, A100, 100 W, 60-80 GB; +grad accum, OOM
    gradient_accumulation_steps: 1
    _cluster_param:
      nnodes: 2
      nproc_per_node: 8
    # eval takes about half an hour
    eval_steps: 0.25
    # save per hour
    save_steps: 500

  Qwen3-Coder-30B-A3B-Instruct:
    model_name_or_path: /mnt/workspace/hf_downloads/Qwen/Qwen3-Coder-30B-A3B-Instruct
    template: qwen3_nothink

    deepspeed: null
    fsdp: hybrid_shard auto_wrap
    fsdp_transformer_layer_cls_to_wrap: Qwen3MoeDecoderLayer

    per_device_train_batch_size: 2
    # bs=1, A100, 100 W, 50-70 GB
    # bs=2, A100, 100 W, 60-80 GB; +grad accum, OOM
    gradient_accumulation_steps: 1
    _cluster_param:
      nnodes: 2
      nproc_per_node: 8
    # eval takes about half an hour
    eval_steps: 0.25
    # save per hour
    save_steps: 500

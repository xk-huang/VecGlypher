# ─────────────────────────────────────────────────────────────
# File: conf/config.yaml  (single flexible config)
# ─────────────────────────────────────────────────────────────
# Keep working directory stable; multirun outputs go here if you sweep later.
hydra:
  run:
    dir: ./misc/submitter_artifacts
  sweep:
    dir: ./misc/submitter_artifacts/multirun/${now:%Y%m%d_%H%M%S}
    subdir: ${hydra.job.num}

# Global toggles
fail_fast: true   # stop on first failed experiment
dry_run: false    # set true to print commands only
jobs: ALL  # comma list or YAML list of exp names; or ALL
local_run: false

# Metadata available to interpolation contexts
meta:
  # run_id: ${now:%y%m%d-%H%M%S}
  run_id: ${now:%y%m%d}
  artifact_root: ./misc/submitter_artifacts


# 1) Shared args for *all* jobs. You can put anything here.
#    Values can be strings, numbers, bools, or lists (lists become comma-joined).
#    Set a key to null in an exp override to remove it from the final CLI.
base_args:
  _cluster_param:
    app: cluster_scripts/cluster.py:train
    host: tc_any
    nnodes: 2
    nproc_per_node: 8
    max_retries: 5
    script: ./cluster_scripts/setup_and_run.sh
    # either use string or list of strings (e.g., ["python", "src/sft/train.py"])
    program: ["python", "src/sft/train.py"]
    config_file: src/sft/configs/train/qwen3_0_6b-full_sft.yaml

  # Paths
  dataset_dir: /mnt/workspace/svg_glyph_llm/data/processed/filtered_sft/250814-oxford_5000-100_fonts-apply_word_sep
  # Default mixture
  dataset: ["train-sample_100", "train-alphanumeric"]
  eval_dataset: ["ood_test-sample_30-contents_600", "ood_test-alphanumeric"]

  # Training hyperparams
  # HF trainer args: https://huggingface.co/docs/transformers/en/main_classes/trainer#transformers.TrainingArguments
  per_device_train_batch_size: 1
  gradient_accumulation_steps: 1
  learning_rate: 5.0e-5
  weight_decay: 0.01

  # num_train_epochs: 5.0
  max_steps: 10000
  warmup_steps: 500

  save_strategy: steps
  save_steps: 0.1

  eval_strategy: steps
  eval_steps: 0.1

  # cutoff_len: 6144

  # add sep token
  add_special_tokens: "<|SEP|>"

  # Output path can depend on exp_name at runtime
  _output_base_dir: /mnt/workspace/svg_glyph_llm/saves/
  # [NOTE] Change this to your own output dir
  _exp_name: 250820-oxford_5000-100_fonts-apply_word_sep-fsdp-filtered
  _job_name: debug
  output_dir: null

# 2) Experiment-specific overrides. Each value is merged into base_args.
#    Use null (~) to delete a key from the final argument list.
# fsdp: https://huggingface.co/docs/transformers/en/main_classes/trainer#transformers.TrainingArguments.fsdp
job_args:
  4b:
    model_name_or_path: /mnt/workspace/hf_downloads/Qwen/Qwen3-4B

    deepspeed: null
    fsdp: hybrid_shard_zero2 auto_wrap
    fsdp_transformer_layer_cls_to_wrap: Qwen3DecoderLayer

    pure_bf16: true # use bf16 for mixed precision training
    # ref: https://github.com/huggingface/trl/blob/46d09bd2408f17605409fb3ee8ba12705add7faa/trl/trainer/sft_config.py#L113C22-L113C29

    per_device_train_batch_size: 4
    gradient_accumulation_steps: 1
    _cluster_param:
      nnodes: 1
      nproc_per_node: 8

  8b:
    model_name_or_path: /mnt/workspace/hf_downloads/Qwen/Qwen3-8B

    deepspeed: null
    fsdp: hybrid_shard_zero2 auto_wrap
    fsdp_transformer_layer_cls_to_wrap: Qwen3DecoderLayer

    pure_bf16: true # use bf16 for mixed precision training
    # ref: https://github.com/huggingface/trl/blob/46d09bd2408f17605409fb3ee8ba12705add7faa/trl/trainer/sft_config.py#L113C22-L113C29

    per_device_train_batch_size: 2
    gradient_accumulation_steps: 1
    _cluster_param:
      nnodes: 2
      nproc_per_node: 8

  14b:
    model_name_or_path: /mnt/workspace/hf_downloads/Qwen/Qwen3-14B

    deepspeed: null
    fsdp: hybrid_shard auto_wrap
    fsdp_transformer_layer_cls_to_wrap: Qwen3DecoderLayer

    pure_bf16: true # use bf16 for mixed precision training
    # ref: https://github.com/huggingface/trl/blob/46d09bd2408f17605409fb3ee8ba12705add7faa/trl/trainer/sft_config.py#L113C22-L113C29

    per_device_train_batch_size: 2
    gradient_accumulation_steps: 1
    _cluster_param:
      nnodes: 2
      nproc_per_node: 8

  32b:
    model_name_or_path: /mnt/workspace/hf_downloads/Qwen/Qwen3-32B

    deepspeed: null
    fsdp: full_shard auto_wrap
    fsdp_transformer_layer_cls_to_wrap: Qwen3DecoderLayer

    pure_bf16: true # use bf16 for mixed precision training
    # ref: https://github.com/huggingface/trl/blob/46d09bd2408f17605409fb3ee8ba12705add7faa/trl/trainer/sft_config.py#L113C22-L113C29

    per_device_train_batch_size: 1
    gradient_accumulation_steps: 1
    _cluster_param:
      nnodes: 4
      nproc_per_node: 8

  8b-lr_1e_5:
    model_name_or_path: /mnt/workspace/hf_downloads/Qwen/Qwen3-8B

    deepspeed: null
    fsdp: hybrid_shard_zero2 auto_wrap
    fsdp_transformer_layer_cls_to_wrap: Qwen3DecoderLayer

    pure_bf16: true # use bf16 for mixed precision training
    # ref: https://github.com/huggingface/trl/blob/46d09bd2408f17605409fb3ee8ba12705add7faa/trl/trainer/sft_config.py#L113C22-L113C29

    learning_rate: 1.0e-5
    per_device_train_batch_size: 4
    gradient_accumulation_steps: 1
    _cluster_param:
      nnodes: 2
      nproc_per_node: 8

# ─────────────────────────────────────────────────────────────
# File: conf/config.yaml  (single flexible config)
# ─────────────────────────────────────────────────────────────
# Keep working directory stable; multirun outputs go here if you sweep later.
hydra:
  run:
    dir: ./misc/submitter_artifacts
  sweep:
    dir: ./misc/submitter_artifacts/multirun/${now:%Y%m%d_%H%M%S}
    subdir: ${hydra.job.num}

# Global toggles
fail_fast: true   # stop on first failed experiment
dry_run: false    # set true to print commands only
jobs: ALL  # comma list or YAML list of exp names; or ALL
local_run: false

# Metadata available to interpolation contexts
meta:
  # run_id: ${now:%y%m%d-%H%M%S}
  run_id: ${now:%y%m%d}
  artifact_root: ./misc/submitter_artifacts


# 1) Shared args for *all* jobs. You can put anything here.
#    Values can be strings, numbers, bools, or lists (lists become comma-joined).
#    Set a key to null in an exp override to remove it from the final CLI.
base_args:
  _cluster_param:
    app: cluster_scripts/cluster.py:train
    host: tc_any
    nnodes: 2
    nproc_per_node: 8
    max_retries: 5
    script: ./cluster_scripts/setup_and_run.sh
    # either use string or list of strings (e.g., ["python", "src/sft/train.py"])
    program: ["python", "src/sft/train.py"]
    config_file: src/sft/configs/train/qwen3_0_6b-full_sft.yaml # can be none
    name: null

  # Paths
  dataset_dir: /mnt/workspace/svg_glyph_llm/data/processed/sft/250813-alphanumeric/
  # Default mixture
  dataset: "train_font_family"
  eval_dataset: "ood_font_family_decon"

  # Training hyperparams
  # HF trainer args: https://huggingface.co/docs/transformers/en/main_classes/trainer#transformers.TrainingArguments
  per_device_train_batch_size: 1
  gradient_accumulation_steps: 1
  learning_rate: 5.0e-5
  weight_decay: 0.01

  num_train_epochs: 5.0
  # max_steps: 100000
  warmup_steps: 500

  save_strategy: epoch
  # save_strategy: steps
  # save_steps: 500

  eval_strategy: steps
  eval_steps: 0.1 # If smaller than 1, will be interpreted as ratio of total training steps.

  # cutoff_len: 6144

  # add sep token
  add_special_tokens: "<|SEP|>"

  # Output path can depend on exp_name at runtime
  _output_base_dir: /mnt/workspace/svg_glyph_llm/saves/
  # [NOTE] Change this to your own output dir
  _exp_name: 250819-alphanumeric-fsdp-base
  _job_name: debug
  output_dir: null # Should be none, override by job_args key

# 2) Experiment-specific overrides. Each value is merged into base_args.
#    Use null (~) to delete a key from the final argument list.
# fsdp: https://huggingface.co/docs/transformers/en/main_classes/trainer#transformers.TrainingArguments.fsdp
job_args:
  4b-base:
    model_name_or_path: /mnt/workspace/hf_downloads/Qwen/Qwen3-4B-Base

    deepspeed: null
    fsdp: hybrid_shard_zero2 auto_wrap
    fsdp_transformer_layer_cls_to_wrap: Qwen3DecoderLayer

    per_device_train_batch_size: 4
    gradient_accumulation_steps: 2
    _cluster_param:
      nnodes: 1
      nproc_per_node: 8

  8b-base:
    model_name_or_path: /mnt/workspace/hf_downloads/Qwen/Qwen3-8B-base

    deepspeed: null
    fsdp: hybrid_shard_zero2 auto_wrap
    fsdp_transformer_layer_cls_to_wrap: Qwen3DecoderLayer

    per_device_train_batch_size: 2
    gradient_accumulation_steps: 2
    _cluster_param:
      nnodes: 2
      nproc_per_node: 8

  14b-base:
    model_name_or_path: /mnt/workspace/hf_downloads/Qwen/Qwen3-14B-base

    deepspeed: null
    fsdp: hybrid_shard auto_wrap
    fsdp_transformer_layer_cls_to_wrap: Qwen3DecoderLayer

    per_device_train_batch_size: 2
    gradient_accumulation_steps: 2
    _cluster_param:
      nnodes: 2
      nproc_per_node: 8

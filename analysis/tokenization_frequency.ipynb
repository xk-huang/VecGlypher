{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "os.chdir(\"/home/vecglypher/codes/svg_glyph_llm/\")\n",
        "os.getcwd()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "\n",
        "storage_base_dir = \"/home/vecglypher/mnt/\"\n",
        "storage_base_dir = Path(storage_base_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer\n",
        "from rich.progress import Progress, track\n",
        "import json\n",
        "from tqdm import tqdm\n",
        "from concurrent.futures import as_completed, ProcessPoolExecutor\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "plt.rcParams[\"font.family\"] = \"DejaVu Sans\"   # or any font installed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model_name_or_path = storage_base_dir / \"workspace/hf_downloads/Qwen/Qwen3-4B\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "WORD_SEP = \"<|SEP|>\"\n",
        "additional_special_tokens = [WORD_SEP]\n",
        "tokenizer.add_special_tokens(\n",
        "    {\"additional_special_tokens\": additional_special_tokens},\n",
        "    replace_additional_special_tokens=False,\n",
        ")\n",
        "print(f\"tokenizer.special_tokens_map: {tokenizer.special_tokens_map}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_data_list(jsonl_dir, verbose=False):\n",
        "    jsonl_dir = Path(jsonl_dir)\n",
        "    jsonl_list = list(jsonl_dir.glob(\"*.jsonl\"))\n",
        "    print(f\"number of jsonl files: {len(jsonl_list)}\")\n",
        "\n",
        "    data_list = []\n",
        "    for jsonl in jsonl_list:\n",
        "        with open(jsonl, \"r\") as f:\n",
        "            for line in f:\n",
        "                data_list.append(json.loads(line))\n",
        "    # validate data\n",
        "    if not verbose:\n",
        "        return data_list\n",
        "\n",
        "    data = data_list[0]\n",
        "\n",
        "    instruction = data[\"instruction\"]\n",
        "    system = data[\"system\"]\n",
        "    output = data[\"output\"]\n",
        "\n",
        "    input_str = f\"{instruction} {system}\"\n",
        "    output_str = output\n",
        "\n",
        "    input_token = tokenizer(input_str, add_special_tokens=True)[\"input_ids\"]\n",
        "    output_token = tokenizer(output_str, add_special_tokens=True)[\"input_ids\"]\n",
        "\n",
        "    decode_input_str = tokenizer.decode(input_token, skip_special_tokens=False)\n",
        "    decode_output_str = tokenizer.decode(output_token, skip_special_tokens=False)\n",
        "    print(f\"[[input_str]] {input_str}\")\n",
        "    print(f\"[[decode_str]] {decode_input_str}\")\n",
        "\n",
        "    print(f\"[[output_str]] {output_str}\")\n",
        "    print(f\"[[decode_str]] {decode_output_str}\")\n",
        "\n",
        "    return data_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def stat_token(data, tokenizer):\n",
        "    instruction = data[\"instruction\"]\n",
        "    system = data[\"system\"]\n",
        "    output = data[\"output\"]\n",
        "\n",
        "    input_str = f\"{instruction} {system}\"\n",
        "    output_str = output\n",
        "\n",
        "    input_token = tokenizer(input_str, add_special_tokens=True)[\"input_ids\"]\n",
        "    output_token = tokenizer(output_str, add_special_tokens=True)[\"input_ids\"]\n",
        "\n",
        "    input_token_len = len(input_token)\n",
        "    output_token_len = len(output_token)\n",
        "\n",
        "    metadata = json.loads(data[\"metadata\"])\n",
        "    content_str = metadata[\"content_str\"].replace(WORD_SEP, '')\n",
        "    return {\n",
        "        \"input_str_len\": len(input_str),\n",
        "        \"input_token_len\": input_token_len,\n",
        "        \"output_str_len\": len(output_str),\n",
        "        \"output_token_len\": output_token_len,\n",
        "        \"content_len\": len(content_str)\n",
        "    }\n",
        "\n",
        "def batch_stat_token(data_list, tokenizer):\n",
        "    token_len_list = []\n",
        "    for data in data_list:\n",
        "        stat = stat_token(data, tokenizer)\n",
        "        token_len_list.append(stat)\n",
        "    return token_len_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def stat_token_from_data_list(data_list, tokenizer):\n",
        "    token_len_list = []\n",
        "    num_workers = 20\n",
        "    batch_size = 10000\n",
        "    with ProcessPoolExecutor(max_workers=num_workers) as executor:\n",
        "        future2data =[]\n",
        "        for start_idx in range(0, len(data_list), batch_size):\n",
        "            end_idx = min(start_idx + batch_size, len(data_list))\n",
        "            data_batch = data_list[start_idx:end_idx]\n",
        "            future = executor.submit(batch_stat_token, data_batch, tokenizer)\n",
        "            future2data.append(future)\n",
        "\n",
        "        # for future in tqdm(as_completed(future2data), total=len(future2data)):\n",
        "        for future in as_completed(future2data):\n",
        "            try:\n",
        "                stat = future.result()\n",
        "                token_len_list.extend(stat)\n",
        "            except Exception as exc:\n",
        "                raise exc\n",
        "    return token_len_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def plot_hist(token_len_df, data_name):\n",
        "    fig, ax = plt.subplots(figsize=(12, 7), nrows=2, ncols=3)\n",
        "    ax[0][0].hist(token_len_df[\"input_str_len\"], bins=100)\n",
        "    ax[0][0].set_title(\"input_str_len\", fontdict={\"fontsize\": 10})\n",
        "    ax[0][1].hist(token_len_df[\"input_token_len\"], bins=100)\n",
        "    ax[0][1].set_title(\"input_token_len\", fontdict={\"fontsize\": 10})\n",
        "    # Add quantile lines\n",
        "    quantiles = [0.25, 0.5, 0.75,0.9, 0.95, 0.99]\n",
        "    quantile_values = token_len_df[\"input_token_len\"].quantile(quantiles)\n",
        "    for q, val in zip(quantiles, quantile_values):\n",
        "        ax[0][1].axvline(x=val, color='r', linestyle='--', alpha=0.7, linewidth=0.8,\n",
        "                         label=f'{int(q*100)}% quantile: {int(val)}')\n",
        "    ax[0][1].legend(fontsize=8)\n",
        "    # scatter plot of content_len vs output_token_len\n",
        "    y_max = max(ax[0][0].get_ylim()[-1], ax[0][1].get_ylim()[1])\n",
        "    x_max = max(ax[0][0].get_xlim()[-1], ax[0][1].get_xlim()[1])\n",
        "    for i in range(2):\n",
        "        ax[0][i].set_ylim(0, y_max)\n",
        "        ax[0][i].set_xlim(0, x_max)\n",
        "\n",
        "\n",
        "\n",
        "    ax[1][0].hist(token_len_df[\"output_str_len\"], bins=100)\n",
        "    ax[1][0].set_title(\"output_str_len\", fontdict={\"fontsize\": 10})\n",
        "    ax[1][1].hist(token_len_df[\"output_token_len\"], bins=100)\n",
        "    ax[1][1].set_title(\"output_token_len\", fontdict={\"fontsize\": 10})\n",
        "    # Add quantile lines\n",
        "    quantiles = [0.25, 0.5, 0.75,0.9, 0.95, 0.99, 1.0]\n",
        "    quantile_values = token_len_df[\"output_token_len\"].quantile(quantiles)\n",
        "    for q, val in zip(quantiles, quantile_values):\n",
        "        ax[1][1].axvline(x=val, color='r', linestyle='--', alpha=0.7, linewidth=0.8,\n",
        "                         label=f'{int(q*100)}% quantile: {int(val)}')\n",
        "    ax[1][1].legend(fontsize=8)\n",
        "    # scatter plot of content_len vs output_token_len\n",
        "    ax[1][2].scatter(token_len_df[\"content_len\"], token_len_df[\"output_token_len\"], s=0.5)\n",
        "    y_max = max(ax[1][0].get_ylim()[-1], ax[1][1].get_ylim()[1])\n",
        "    x_max = max(ax[1][0].get_xlim()[-1], ax[1][1].get_xlim()[1])\n",
        "    for i in range(2):\n",
        "        ax[1][i].set_ylim(0, y_max)\n",
        "        ax[1][i].set_xlim(0, x_max)\n",
        "    for ax_ in ax.flatten():\n",
        "        ax_.set_xlabel(\"length\", fontsize=8)\n",
        "        ax_.set_ylabel(\"count\", fontsize=8)\n",
        "        ax_.tick_params(axis='both', which='major', labelsize=8)\n",
        "\n",
        "    ax[1][2].set_xlabel(\"content_len\")\n",
        "    ax[1][2].set_ylabel(\"output_token_len\")\n",
        "    ax[1][2].set_xlim(0, ax[1][2].get_xlim()[-1])\n",
        "    ax[1][2].set_ylim(0, ax[1][2].get_ylim()[-1])\n",
        "\n",
        "\n",
        "\n",
        "    fig_title = f\"token/string length distribution {data_name}\"\n",
        "    fig.suptitle(fig_title)\n",
        "\n",
        "    output_dir = Path(\"misc/token_length\")\n",
        "    output_dir.mkdir(parents=True, exist_ok=True)\n",
        "    output_path = output_dir / f\"{data_name}.png\"\n",
        "    plt.savefig(output_path, dpi=300, bbox_inches=\"tight\")\n",
        "    print(f\"save to {output_path}\")\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pairs = [\n",
        "\"ood_font_family\",\n",
        "\"train_font_family\",\n",
        "]\n",
        "# jsonl_base_dir = storage_base_dir / \"workspace/svg_glyph_llm/data/processed/sft/250814-oxford_5000-100_fonts-apply_word_sep\"\n",
        "jsonl_base_dir = \"data/processed_envato/sft/250903-envato-alphanumeric\"\n",
        "for data_name in pairs:\n",
        "    jsonl_dir = Path(jsonl_base_dir) / data_name\n",
        "    print(f\"jsonl_dir: {jsonl_dir}\")\n",
        "    data_list = load_data_list(jsonl_dir)\n",
        "\n",
        "    token_len_list = stat_token_from_data_list(data_list, tokenizer)\n",
        "\n",
        "    token_len_df = pd.DataFrame(token_len_list)\n",
        "    token_len_df.head()\n",
        "\n",
        "    plot_title = f\"{Path(jsonl_base_dir).name}-{data_name}\"\n",
        "    plot_hist(token_len_df, plot_title)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pairs = [\n",
        "\"ood_font_family\",\n",
        "\"train_font_family\",\n",
        "]\n",
        "# jsonl_base_dir = storage_base_dir / \"workspace/svg_glyph_llm/data/processed/sft/250814-oxford_5000-100_fonts-apply_word_sep\"\n",
        "jsonl_base_dir = storage_base_dir / \"workspace/svg_glyph_llm/data/processed/sft/250903-alphanumeric/\"\n",
        "for data_name in pairs:\n",
        "    jsonl_dir = Path(jsonl_base_dir) / data_name\n",
        "    print(f\"jsonl_dir: {jsonl_dir}\")\n",
        "    data_list = load_data_list(jsonl_dir)\n",
        "\n",
        "    token_len_list = stat_token_from_data_list(data_list, tokenizer)\n",
        "\n",
        "    token_len_df = pd.DataFrame(token_len_list)\n",
        "    token_len_df.head()\n",
        "\n",
        "    plot_title = f\"{Path(jsonl_base_dir).name}-{data_name}\"\n",
        "    plot_hist(token_len_df, plot_title)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pairs = [\n",
        "\"train-alphanumeric\",\n",
        "\"ood_test-alphanumeric\",\n",
        "\"train-sample_100\",\n",
        "\"ood_test-sample_30-contents_600\",\n",
        "]\n",
        "jsonl_base_dir = storage_base_dir / \"workspace/svg_glyph_llm/data/processed/sft/250903-oxford_5000-100_fonts-apply_word_sep\"\n",
        "for data_name in pairs:\n",
        "    jsonl_dir = Path(jsonl_base_dir) / data_name\n",
        "    print(f\"jsonl_dir: {jsonl_dir}\")\n",
        "    data_list = load_data_list(jsonl_dir)\n",
        "\n",
        "    token_len_list = stat_token_from_data_list(data_list, tokenizer)\n",
        "\n",
        "    token_len_df = pd.DataFrame(token_len_list)\n",
        "    token_len_df.head()\n",
        "\n",
        "    plot_title = f\"{Path(jsonl_base_dir).name}-{data_name}\"\n",
        "    plot_hist(token_len_df, plot_title)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "fileHeader": "",
    "fileUid": "93400890-9a91-4e85-84c6-4cc69320f320",
    "isAdHoc": false,
    "kernelspec": {
      "display_name": "svg_glyph_llm (conda)",
      "language": "python",
      "name": "conda_svg_glyph_llm"
    },
    "language_info": {
      "name": "plaintext"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
